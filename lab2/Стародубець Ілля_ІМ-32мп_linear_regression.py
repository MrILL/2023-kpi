import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

np.random.seed(123)


def parameters_inititalization(m):
    """
    Ця функція ініціалізує вектор-рядок випадкових дійсних значень ваг форми (1, m),
    отриманих з нормального розподілу та зсув (довільне дійсне значення)

    Параметри:
    m -- кількість вхідних ознак для кожного навчального прикладу

    Повертає:
    W -- вектор-рядок ваг форми (1, m)
    b -- зсув (скаляр)
    """

    # BEGIN_YOUR_CODE
    W = np.random.randn(1, m)
    b = 0.0

    return W, b
    # END_YOUR_CODE


def forwardPropagate(X, W, b):
    """
    Ця функція обчислює лінійну комбінацію вхідних ознак та ваг, включаючи зсув

    Параметри:
    X -- вхідний вектор ознак форми (1, X_train.shape[1])
    W -- вектор-рядок ваг форми (1, m)
    b -- зсув моделі (скаляр)

    Повертає:
    z -- загальна зважена сума вхідних ознак, включаючи зсув
    y_hat -- прогноз моделі
    """

    # BEGIN_YOUR_CODE
    z = np.dot(W, X) + b
    y_hat = z
    return z, y_hat
    # END_YOUR_CODE


def cost(n, y_hat, y_true):
    """
    Ця функція обчислює середнє квадратичне відхилення на всьому навчальному наборі даних

    Параметри:
    n -- загальна кількість навчальних прикладів
    y_hat -- вихідне значення лінійної регресії
    y_true -- істинне значення залежної змінної

    Повертає:
    J --  середнє квадратичне відхилення на всьому навчальному наборі даних
    """

    # BEGIN_YOUR_CODE
    J = (1 / n) * np.sum((y_hat - y_true) ** 2)
    return J
    # END_YOUR_CODE


def backwardPropagate(n, X, y_hat, y_true):
    """
    Ця функція обчислює градієнти цільвої функції відносно ваг та зсуву

    Параметри:
    n -- загальна кількість навчальних прикладів
    X -- вхідний вектор ознак форми (1, X_train.shape[1])
    y_hat --  вихідне значення лінійної регресії
    y_true -- істинне значення залежної змінної

    Повертає:
    dW --  градієнт цільової функції відносно ваг моделі
    db -- градієнт цільової функції відносно зсуву моделі
    """

    # BEGIN_YOUR_CODE
    dW = (2 / n) * np.dot((y_hat - y_true), X.T)
    db = (2 / n) * np.sum(y_hat - y_true)
    return dW, db
    # END_YOUR_CODE


def update(alpha, dW, db, W, b):
    """
    Ця функція оновлює навчальні параметри моделі (ваги та зсув ) у напрямку мінімізації цільової функції

    Параметри:
    alpha -- швидкість  навчання (крок навчання)
    dW --  градієнт цільової функції відносно ваг моделі
    db -- градієнт цільової функції відносно зсуву моделі
    W -- вектор-рядок ваг моделі форми (1, m)
    b -- зсув моделі (скаляр)

    Повертає:
    W -- оновлений вектор-рядок ваг моделі форми (1, m)
    b -- оновлений зсув моделі (скаляр)
    """

    # BEGIN_YOUR_CODE
    W -= alpha * dW
    b -= alpha * db
    return W, b
    # END_YOUR_CODE
